{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC:\n",
    "    def __init__(self, feature_types, num_classes):\n",
    "        self.feature_types = feature_types \n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        classes, classCounts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        pis = np.expand_dims(classCounts / y.shape[0], axis=1)\n",
    "        \n",
    "        allMus = [] \n",
    "        allSigmaSqrs = []\n",
    "        \n",
    "        for i in range(classes.shape[0]):\n",
    "            # Get class examples  \n",
    "            classExampleIdxs = np.argwhere(y==classes[i])[:,0]\n",
    "        \n",
    "            # Calculate class parameters\n",
    "            # Empirical mean / Bernoulli distribution parameter\n",
    "            mus = np.average(X[classExampleIdxs,:], axis=0)\n",
    "            # Emperical variance \n",
    "            sigmaSqrs = np.var(X[classExampleIdxs,:], axis=0)\n",
    "            \n",
    "            allMus.append(mus)\n",
    "            allSigmaSqrs.append(sigmaSqrs)\n",
    "        \n",
    "        allMus = np.array(allMus)\n",
    "        allSigmaSqrs = np.array(allSigmaSqrs)\n",
    "        \n",
    "        self.pis = pis\n",
    "        self.mus = allMus\n",
    "        self.sigmaSqrs = allSigmaSqrs\n",
    "        self.classes = classes\n",
    "    \n",
    "    def calcRealProb(self, X, realIndices, clsIdx): \n",
    "        mean = self.mus[clsIdx,realIndices]\n",
    "        variances = self.sigmaSqrs[clsIdx,realIndices] \n",
    "        realXs = X[:,realIndices]\n",
    "        probs = np.exp(-np.square(realXs-mean)/(2*(variances+1e-6)))/np.sqrt(2*np.pi*(variances+1e-6))\n",
    "        return probs\n",
    "    \n",
    "    def calcCatProb(self, X, binIndices, clsIdx):\n",
    "        params = self.mus[clsIdx,binIndices]\n",
    "        binXs = X[:,binIndices]\n",
    "        probs = np.zeros_like(binXs) \n",
    "        for i in range(params.shape[0]):\n",
    "            feature = binXs[:,i]\n",
    "            featureProbs = np.zeros_like(feature)\n",
    "            featureProbs[feature == 1] = params[i]\n",
    "            featureProbs[feature == 0] = 1 - params[i]\n",
    "            probs[:,i] = featureProbs\n",
    "        return probs\n",
    "        \n",
    "    def calcClassProb(self, X, clsIdx):\n",
    "        featureTypes = self.feature_types\n",
    "        binIndices = [i for i, x in enumerate(featureTypes) if x == 'b']\n",
    "        realIndices = [i for i, x in enumerate(featureTypes) if x == 'r']\n",
    "        \n",
    "        realProbs = self.calcRealProb(X, realIndices, clsIdx)\n",
    "        catProbs = self.calcCatProb(X, binIndices, clsIdx)\n",
    "        \n",
    "        # Ensure no zeros\n",
    "        realProbs[realProbs == 0] = 1e-6\n",
    "        catProbs[catProbs == 0] = 1e-6\n",
    "        \n",
    "        realProbs = np.log(realProbs)\n",
    "        catProbs = np.log(catProbs)\n",
    "        realProbs = np.sum(realProbs,axis=1)\n",
    "        catProbs = np.sum(catProbs, axis=1)\n",
    "        \n",
    "        return realProbs + catProbs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        classProbs = []\n",
    "        for i in range(self.classes.shape[0]):\n",
    "            classProbs.append(self.calcClassProb(X, i))\n",
    "        classProbs = np.array(classProbs)\n",
    "        predictedClassIdx = np.argmax(classProbs,axis=0)\n",
    "        return self.classes[predictedClassIdx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]\n",
      " [0.5]]\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "nbc = NBC(feature_types=['b','r','b','r'], num_classes=2)\n",
    "nbc.fit(np.array([[[1],[0.5],[1],[0.5]],[[1],[0.5],[0],[0.5]],[[1],[0.5],[0],[0.5]]]),np.array([[2],[0],[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 1)\n",
      "(3, 1)\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[[1],[0.5],[1],[0.5]],[[1],[0.5],[0],[0.5]],[[1],[0.5],[0],[0.5]]])\n",
    "print(X.shape)\n",
    "yhat = nbc.predict(X)\n",
    "y = np.array([[2],[0],[2]])\n",
    "print(y.shape)\n",
    "test_accuracy = np.mean(y == yhat)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy 0.9553571428571429\n",
      "Naive Bayes Test Accuracy 0.9736842105263158\n",
      "Logistic Regression Training Accuracy 0.3335459183673469\n",
      "Logistic Regression Test Accuracy 0.3365650969529086\n"
     ]
    }
   ],
   "source": [
    "# Iris Dataset \n",
    "iris = load_iris() \n",
    "X, y = iris['data'], iris['target']\n",
    "XTrain, XTest, yTrain, yTest =\\\n",
    "    train_test_split(X, y, test_size=0.25)\n",
    "XTrain = np.expand_dims(XTrain,axis=2)\n",
    "XTest = np.expand_dims(XTest,axis=2)\n",
    "yTrain = np.expand_dims(yTrain,axis=1)\n",
    "yTest = np.expand_dims(yTest,axis=1)\n",
    "\n",
    "nbc = NBC(feature_types=['r','r','r','r'], num_classes=3)\n",
    "nbc.fit(XTrain, yTrain)\n",
    "nbcYTrainPredict = nbc.predict(XTrain)\n",
    "nbcYTestPredict = nbc.predict(XTest)\n",
    "nbcTrainAccuracy = np.mean(nbcYTrainPredict == yTrain)\n",
    "nbcTestAccuracy = np.mean(nbcYTestPredict == yTest)\n",
    "\n",
    "logReg = LogisticRegression(\\\n",
    "    solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "logReg.fit(np.squeeze(XTrain), np.squeeze(yTrain))\n",
    "logRegYTrainPredict = logReg.predict(np.squeeze(XTrain))\n",
    "logRegYTestPredict = logReg.predict(np.squeeze(XTest))\n",
    "logRegTrainAccuracy = np.mean(logRegYTrainPredict == yTrain)\n",
    "logRegTestAccuracy = np.mean(logRegYTestPredict == yTest)\n",
    "\n",
    "print(\"Naive Bayes Training Accuracy\", nbcTrainAccuracy)\n",
    "print(\"Naive Bayes Test Accuracy\", nbcTestAccuracy)\n",
    "print(\"Logistic Regression Training Accuracy\", logRegTrainAccuracy)\n",
    "print(\"Logistic Regression Test Accuracy\", logRegTestAccuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
